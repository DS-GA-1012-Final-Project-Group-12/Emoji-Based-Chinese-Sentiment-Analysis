{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn\n",
    "#!pip install transformers\n",
    "#!pip install -U ray\n",
    "#!pip install -U ray[tune]\n",
    "#!pip install datasets\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "from cf_matrix import make_confusion_matrix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji from data\n",
    "def no_emoji(X):\n",
    "    for i in range(len(X)):\n",
    "        s = ''\n",
    "        count = 0\n",
    "        for j in range(len(X[i])):\n",
    "            if X[i][j] == \"[\":\n",
    "                count += 1\n",
    "            elif count == 0:\n",
    "                s += X[i][j]\n",
    "            if X[i][j] == \"]\" and count > 0:\n",
    "                count -= 1\n",
    "        X[i] = s\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "# split data to train, validation, test\n",
    "def split(df, need_emoji = True, random_state = 0):\n",
    "    X = list(df['review'])\n",
    "    y = list(df['label'])\n",
    "    \n",
    "    # 60% train, 20% development, 20% test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = random_state)\n",
    "    if not need_emoji:\n",
    "        X_train = no_emoji(X_train)\n",
    "        X_val = no_emoji(X_val)\n",
    "        X_test = no_emoji(X_test)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weibo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weibo data\n",
    "df_weibo = pd.read_csv('data/processed_weibo_data.csv')\n",
    "df_weibo = df_weibo.dropna().drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split weibo data to train, validation, test\n",
    "# data with emoji\n",
    "X_train_wb_1, X_val_wb_1, X_test_wb_1, y_train_wb_1, y_val_wb_1, y_test_wb_1 = split(df_weibo)\n",
    "# same data with emoji removed\n",
    "X_train_wb_0, X_val_wb_0, X_test_wb_0, y_train_wb_0, y_val_wb_0, y_test_wb_0 = split(df_weibo, need_emoji = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['猛然发现，「财经郎眼」之前的广告是修正牌消糜栓。[衰]',\n",
       "  '看爪就知道是美女！[亲亲][爱你]',\n",
       "  '你现在是在去怀柔的路上吗？[哈哈][哈哈] ',\n",
       "  '时间过得太快，老感觉不够用！[泪]',\n",
       "  '吃饱喝足谁也不服[哈哈][哈哈][哈哈]'],\n",
       " ['猛然发现，「财经郎眼」之前的广告是修正牌消糜栓。',\n",
       "  '看爪就知道是美女！',\n",
       "  '你现在是在去怀柔的路上吗？ ',\n",
       "  '时间过得太快，老感觉不够用！',\n",
       "  '吃饱喝足谁也不服']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_train_wb_1[0:5], X_train_wb_0[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dianping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dianping data\n",
    "df_dianping = pd.read_csv('data/processed_dianping_data.csv')\n",
    "df_dianping = df_dianping.dropna().drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dianping data to train, validation, test\n",
    "# dianping data does not have emoji\n",
    "X_train_dp_0, X_val_dp_0, X_test_dp_0, y_train_dp_0, y_val_dp_0, y_test_dp_0 = split(df_dianping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中午在旺角逛街，12点多就去了附近的潮楼。那里应该算是早茶的地方的，但是中午人依然很多。还好到的早，不怎么需要等位的。领位的服务生，忙是忙，一时忽略了我们，还打招呼，不好意思。\\n绫鱼丸，新鲜鱼肉做的，能吃的出有鱼刺的成分，但是绝对不会卡到的那种，很鲜；\\n皮蛋瘦肉粥，好浓的，以至于我和V都说，要么回家以后也把粥煮成这样，但是味道肯定调不好的；\\n虾饺，里面真的有3个虾仁，很Q的，要不是吃不下，肯定还会再点的；\\n凤爪，比较酥烂，食材本身研制的比较入味，所以很好吃，V说比较适合她妈吃；\\n肠粉，里面的料很好，也有虾仁的成分面还有新鲜蔬菜，结合的好香；\\nXO酱萝卜糕，因为便宜就点来吃吃，绝对超过了萝卜的口感，超级好吃；\\n牛肉丸，做的比较嫩，但是确很Q，跟芹菜结合的，有种特殊的香味。\\n最后结帐$123，出乎意料，本来还以为要超200了。这顿真的吃的很舒服。\\n',\n",
       " '鸟照烧 大家都懂了对伐 20块钱的中饭 还是蛮灵的\\n就是感觉很 穷人的感觉，几片腌萝卜 还有鸟照烧 。\\n米饭还是蛮好吃的\\xa0\\xa0MISO有点咸。\\n店很古老了....\\n',\n",
       " '也是从网上看到这个地方的推荐，找了半天都没有找到。瓷器口进去大概是个T型号的路，从最下面进去，往T的丁字口往右走就是江边。快到丁字口的路上，在右边有个非常窄的巷子，不注意就错过。进去才发现别有洞天，里面位置挺宽敞的，里面有个小牌子说是CCTV2推荐的。我们专程点了毛血旺，网上人说这个很地道。这边的毛血旺的确是最老方式的毛血旺，不过味道不太好，感觉重庆城里随便一个摊位上的都比这好吃。还好，点了一个折耳根还下饭，要不然真不知道吃什么。建议，没有什么必要就不要找这个地方吧。\\n',\n",
       " '一直很怀念那儿的鸡，不知道是怎么做的，但是真的很嫩，每次去人基本上都客满的，还有各种炒菜，只是店小了点，值得一试\\n',\n",
       " '南非世界杯，2点钟德国的半决赛是在这里看的。\\n我觉得麦当劳比肯德基好，最近几年扩张后劲十足，反观肯德基似乎孱弱并且诟病不断，推出不少纯噱头难吃的要死的骗钱食品。\\n加之麦咖啡进驻，麦当劳让越来越多人喜欢。\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dp_0[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('uer/chinese_roberta_L-12_H-768')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with weibo emoji\n",
    "roberta_weibo_emoji = AutoModelForSequenceClassification.from_pretrained('trained_model/roberta_weibo_emoji')\n",
    "# classifier\n",
    "weibo_emoji_classifier = pipeline('sentiment-analysis', model = roberta_weibo_emoji, tokenizer = tokenizer, device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without weibo emoji\n",
    "roberta_weibo_no_emoji = AutoModelForSequenceClassification.from_pretrained('trained_model/roberta_weibo_no_emoji')\n",
    "# classifier\n",
    "weibo_no_emoji_classifier = pipeline('sentiment-analysis', model = roberta_weibo_no_emoji, tokenizer = tokenizer, device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uer dianping model\n",
    "uer_roberta_dianping = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-dianping-chinese')\n",
    "# classifier\n",
    "uer_dianping_classifier = pipeline('sentiment-analysis', model = uer_roberta_dianping, tokenizer = tokenizer, device = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess performance of model, and print a confusion matrix\n",
    "def performance(X_test, y_test, classifier, threshold = 0.5):\n",
    "    #convert = {'positive (stars 4 and 5)': 1, 'negative (stars 1, 2 and 3)': 0}\n",
    "    convert = {'LABEL_1': 1, 'LABEL_0': 0}\n",
    "    y_pred = []\n",
    "    \n",
    "    for review in X_test:\n",
    "        prediction = classifier(review)[0]\n",
    "        label = convert[prediction['label']]\n",
    "        if label == 1 and prediction['score'] < threshold:\n",
    "            label = 0\n",
    "        y_pred.append(label)\n",
    "        clear_output(wait = True)\n",
    "        print(\"{}/{}\".format(len(y_pred), len(y_test)))\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    labels = ['TN', 'FP', 'FN', 'TP']\n",
    "    categories = ['Negative', 'Positive']\n",
    "    \n",
    "    make_confusion_matrix(cf_matrix, group_names = labels, categories = categories, cmap = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weibo emoji model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji model performance on weibo data with emoji\n",
    "performance(X_test_wb_1, y_test_wb_1, weibo_emoji_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji model performance on weibo data with emoji removed\n",
    "performance(X_test_wb_0, y_test_wb_0, weibo_emoji_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji model performance on dianping data\n",
    "performance(X_test_dp_0, y_test_dp_0, weibo_emoji_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weibo no emoji model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non emoji model performance on weibo data with emoji\n",
    "performance(X_test_wb_1, y_test_wb_1, weibo_no_emoji_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non emoji model performance on weibo data with emoji removed\n",
    "performance(X_test_wb_0, y_test_wb_0, weibo_no_emoji_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non emoji model performance on dianping data\n",
    "performance(X_test_dp_0, y_test_dp_0, weibo_no_emoji_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dianping model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess performance of model, and print a confusion matrix\n",
    "def performance(X_test, y_test, classifier, threshold = 0.5):\n",
    "    convert = {'positive (stars 4 and 5)': 1, 'negative (stars 1, 2 and 3)': 0}\n",
    "    #convert = {'LABEL_1': 1, 'LABEL_0': 0}\n",
    "    y_pred = []\n",
    "    \n",
    "    for review in X_test:\n",
    "        prediction = classifier(review)[0]\n",
    "        label = convert[prediction['label']]\n",
    "        if label == 1 and prediction['score'] < threshold:\n",
    "            label = 0\n",
    "        y_pred.append(label)\n",
    "        clear_output(wait = True)\n",
    "        print(\"{}/{}\".format(len(y_pred), len(y_test)))\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    labels = ['TN', 'FP', 'FN', 'TP']\n",
    "    categories = ['Negative', 'Positive']\n",
    "    \n",
    "    make_confusion_matrix(cf_matrix, group_names = labels, categories = categories, cmap = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dianping model performance on weibo data with emoji\n",
    "performance(X_test_wb_1, y_test_wb_1, uer_dianping_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dianping model performance on weibo data with emoji removed\n",
    "performance(X_test_wb_0, y_test_wb_0, uer_dianping_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dianping model performance on dianping data\n",
    "performance(X_test_dp_0, y_test_dp_0, uer_dianping_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Misclassified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_idx(data, target, classifier):\n",
    "    threshold = 0.5\n",
    "    convert = {'LABEL_1': 1, 'LABEL_0': 0}\n",
    "    y_pred = []\n",
    "\n",
    "    for review in data:\n",
    "        prediction = classifier(review)[0]\n",
    "        label = convert[prediction['label']]\n",
    "        if label == 1 and prediction['score'] < threshold:\n",
    "            label = 0\n",
    "        y_pred.append(label)\n",
    "    \n",
    "    tp = (np.array(y_pred) + np.array(target)) == 2\n",
    "    tn = (np.array(y_pred) + np.array(target)) == 0\n",
    "    fp = (np.array(y_pred) + 2 * np.array(target)) == 1\n",
    "    fn = (np.array(y_pred) + 2 * np.array(target)) == 2\n",
    "\n",
    "    tp_idx = np.where(tp == True)[0]\n",
    "    tn_idx = np.where(tn == True)[0]\n",
    "    fp_idx = np.where(fp == True)[0]\n",
    "    fn_idx = np.where(fn == True)[0]\n",
    "    \n",
    "    return tp_idx, tn_idx, fp_idx, fn_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weibo review with emoji on weibo emoji model\n",
    "y_test_wb_1_emoji_metrics = metrics_idx(X_test_wb_1, y_test_wb_1, weibo_emoji_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weibo review without emoji on weibo emoji model\n",
    "y_test_wb_0_emoji_metrics = metrics_idx(X_test_wb_0, y_test_wb_0, weibo_emoji_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_tp_emoji_diff = np.setdiff1d(y_test_wb_1_emoji_metrics[0], y_test_wb_0_emoji_metrics[0])\n",
    "np.array(X_test_wb_1)[wb_tp_emoji_diff][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_tn_emoji_diff = np.setdiff1d(y_test_wb_1_emoji_metrics[1], y_test_wb_0_emoji_metrics[1])\n",
    "np.array(X_test_wb_1)[wb_tn_emoji_diff][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_tp_emoji_diff_rev = np.setdiff1d(y_test_wb_0_emoji_metrics[0], y_test_wb_1_emoji_metrics[0])\n",
    "np.array(X_test_wb_1)[wb_tp_emoji_diff_rev][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_tn_emoji_diff_rev = np.setdiff1d(y_test_wb_0_emoji_metrics[1], y_test_wb_1_emoji_metrics[1])\n",
    "np.array(X_test_wb_1)[wb_tn_emoji_diff_rev][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_emoji_classifier(\"小何，你觉得呢？[懒得理你]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_emoji_classifier(\"小何，你觉得呢？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_no_emoji_classifier(\"小何，你觉得呢？[懒得理你]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibo_no_emoji_classifier(\"小何，你觉得呢？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uer_dianping_classifier(\"小何，你觉得呢？[懒得理你]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uer_dianping_classifier(\"小何，你觉得呢？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 5% of data has length less than or equal to 1 when emoji is removed, so they may be ambiguous when classifying. We set a threshold of 10 to filter out short texts, and re-assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove texts with length shorter than 10\n",
    "X_test_1_filtered = []\n",
    "y_test_1_filtered = []\n",
    "X_test_2_filtered = []\n",
    "y_test_2_filtered = []\n",
    "\n",
    "for i in range(len(X_test_2)):\n",
    "    if len(X_test_2[i]) >= 10:\n",
    "        X_test_1_filtered.append(X_test_1[i])\n",
    "        y_test_1_filtered.append(y_test_1[i])\n",
    "        X_test_2_filtered.append(X_test_2[i])\n",
    "        y_test_2_filtered.append(y_test_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
